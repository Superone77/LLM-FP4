import numpy as np
import torch
import os
from importlib import reload,import_module

def init_config(config_name):
    """initialize the config. Use reload to make sure it's fresh one!"""
    _,_,files =  next(os.walk("./configs"))
    if config_name+".py" in files:
        quant_cfg = import_module(f"configs.{config_name}")
    else:
        raise NotImplementedError(f"Invalid config name {config_name}")
    reload(quant_cfg)
    return quant_cfg

class cfg_modifier():
    def __init__(self, **kwargs):
        for name, value in kwargs.items():
            setattr(self,name,value)

    def __call__(self, cfg):
        # bit setting
        cfg.bit = self.bit_setting
        cfg.w_bit = {name: self.bit_setting[0] for name in cfg.fc_name_list}
        cfg.a_bit = {name: self.bit_setting[1] for name in cfg.fc_name_list}
        cfg.embed_bit = {name: self.bit_setting[2] for name in cfg.embed_name_list}
        cfg.A_bit = {name: self.bit_setting[1] for name in cfg.matmul_name_list}
        cfg.B_bit = {name: self.bit_setting[1] for name in cfg.matmul_name_list}
        cfg.w_exp_bit = {name: self.bit_setting[3] for name in cfg.fc_name_list}
        cfg.a_exp_bit = {name: self.bit_setting[4] for name in cfg.fc_name_list}
        cfg.embed_exp_bit = {name: self.bit_setting[5] for name in cfg.embed_name_list}
        cfg.A_exp_bit = {name: self.bit_setting[4] for name in cfg.matmul_name_list}
        cfg.B_exp_bit = {name: self.bit_setting[4] for name in cfg.matmul_name_list}


        cfg.ptqsl_linear_kwargs["eq_alpha"] = self.search_intervals[0]
        cfg.ptqsl_linear_kwargs["eq_beta"] = self.search_intervals[1]
        cfg.ptqsl_linear_kwargs["eq_n"] = self.search_intervals[2]
        cfg.ptqsl_embedding_kwargs["eq_alpha"] = self.search_intervals[0]
        cfg.ptqsl_embedding_kwargs["eq_beta"] = self.search_intervals[1]
        cfg.ptqsl_embedding_kwargs["eq_n"] = self.search_intervals[2]


        cfg.ptqsl_linear_kwargs["search_round"] = self.search_round
        cfg.ptqsl_embedding_kwargs["search_round"] = self.search_round

        return cfg


def set_seed(seed):
    np.random.seed(seed)
    torch.random.manual_seed(seed)


def get_wikitext2(nsamples, seed, seqlen, model):
    from datasets import load_dataset
    traindata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')
    testdata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')

    from transformers import AutoTokenizer 
    tokenizer = AutoTokenizer.from_pretrained(model, use_fast=True)
    trainenc = tokenizer("\n\n".join(traindata['text']), return_tensors='pt')
    testenc = tokenizer("\n\n".join(testdata['text']), return_tensors='pt')

    import random
    random.seed(seed)
    trainloader = []
    for _ in range(nsamples):
        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)
        j = i + seqlen
        inp = trainenc.input_ids[:, i:j]
        tar = inp.clone()
        tar[:, :-1] = -100
        trainloader.append((inp, tar))
    return trainloader, testenc

def get_ptb(nsamples, seed, seqlen, model):
    from datasets import load_dataset
    traindata = load_dataset('ptb_text_only', 'penn_treebank', split='train')
    valdata = load_dataset('ptb_text_only', 'penn_treebank', split='validation')

    from transformers import AutoTokenizer 
    tokenizer = AutoTokenizer.from_pretrained(model, use_fast=True)
    trainenc = tokenizer("\n\n".join(traindata['sentence']), return_tensors='pt')
    testenc = tokenizer("\n\n".join(valdata['sentence']), return_tensors='pt')

    import random
    random.seed(seed)
    trainloader = []
    for _ in range(nsamples):
        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)
        j = i + seqlen
        inp = trainenc.input_ids[:, i:j]
        tar = inp.clone()
        tar[:, :-1] = -100
        trainloader.append((inp, tar))
    return trainloader, testenc

def get_c4(nsamples, seed, seqlen, model):
    from datasets import load_dataset
    traindata = load_dataset("allenai/c4", "en", split="train[:1%]", cache_dir = "./c4_cache")
    # traindata = load_dataset(
    #     'json', data_files='/local/mnt/workspace/wanqi/tmp/data/c4/en/c4-train.00000-of-01024.json.gz'
    # )
    valdata = load_dataset("allenai/c4", "en", split="validation", cache_dir = "./c4_cache")

    # valdata = load_dataset(
    #     'json', data_files='/local/mnt/workspace/wanqi/tmp/data/c4/en/c4-validation.00000-of-00008.json.gz'
    # )

    from transformers import AutoTokenizer
    tokenizer = AutoTokenizer.from_pretrained(model, use_fast=True)

    import random
    random.seed(seed)
    trainloader = []
    for _ in range(nsamples):
        while True:
            i = random.randint(0, len(traindata) - 1)
            trainenc = tokenizer(traindata[i]['text'], return_tensors='pt')
            if trainenc.input_ids.shape[1] >= seqlen:
                break
        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)
        j = i + seqlen
        inp = trainenc.input_ids[:, i:j]
        tar = inp.clone()
        tar[:, :-1] = -100
        trainloader.append((inp, tar))

    import random
    random.seed(0)
    valenc = []
    for _ in range(256):
        while True:
            i = random.randint(0, len(valdata) - 1)
            tmp = tokenizer(valdata[i]['text'], return_tensors='pt')
            if tmp.input_ids.shape[1] >= seqlen:
                break
        i = random.randint(0, tmp.input_ids.shape[1] - seqlen - 1)
        j = i + seqlen
        valenc.append(tmp.input_ids[:, i:j])
    valenc = torch.hstack(valenc)
    class TokenizerWrapper:
        def __init__(self, input_ids):
            self.input_ids = input_ids
    valenc = TokenizerWrapper(valenc)

    return trainloader, valenc 


def get_loaders(
    name, nsamples=128, seed=0, seqlen=2048, model=''
):
    if 'wikitext2' in name:
        return get_wikitext2(nsamples, seed, seqlen, model)
    if 'ptb' in name:
        return get_ptb(nsamples, seed, seqlen, model)
    if 'c4' in name:
        return get_c4(nsamples, seed, seqlen, model)
